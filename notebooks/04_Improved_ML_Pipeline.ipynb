{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved XGBoost Pipeline with SHAP Explainability\n",
    "\n",
    "This notebook implements the improved ML pipeline with all Phase 1 and Phase 2 enhancements:\n",
    "\n",
    "**Phase 1 (Critical):**\n",
    "- ‚úÖ Data validation & feature consistency checks\n",
    "- ‚úÖ Separate validation set (no data leakage)\n",
    "- ‚úÖ SHAP values for model interpretability\n",
    "- ‚úÖ Git commit tracking\n",
    "- ‚úÖ Environment metadata\n",
    "\n",
    "**Phase 2 (Enhanced):**\n",
    "- ‚úÖ JSON configuration\n",
    "- ‚úÖ Learning curves\n",
    "- ‚úÖ Adjusted R¬≤\n",
    "- ‚úÖ Correlation heatmap\n",
    "- ‚úÖ QQ plots for residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "# Import our improved utilities\n",
    "from src.ml_utils import (\n",
    "    validate_train_test_features,\n",
    "    check_missing_values,\n",
    "    adjusted_r2,\n",
    "    create_model_metadata,\n",
    "    save_model_with_metadata,\n",
    "    load_config,\n",
    "    log_data_split_info\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries and utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing train/test split\n",
    "X_train_full = pd.read_parquet('../data/processed/X_train.parquet')\n",
    "X_test = pd.read_parquet('../data/processed/X_test.parquet')\n",
    "y_train_full = pd.read_parquet('../data/processed/y_train.parquet').squeeze()\n",
    "y_test = pd.read_parquet('../data/processed/y_test.parquet').squeeze()\n",
    "\n",
    "print(f\"Original split:\")\n",
    "print(f\"  Train: X={X_train_full.shape}, y={y_train_full.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\nFeatures: {X_train_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PHASE 1: Create Separate Validation Set\n",
    "\n",
    "Split training data into train (80%) and validation (20%) to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_full into train + validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Log split information\n",
    "log_data_split_info(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PHASE 1: Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç PHASE 1: Data Validation\\n\")\n",
    "\n",
    "# Validate feature consistency\n",
    "validate_train_test_features(X_train, X_test)\n",
    "validate_train_test_features(X_train, X_val)\n",
    "\n",
    "# Check for missing values\n",
    "check_missing_values(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PHASE 2: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyperparameters from JSON config\n",
    "params = load_config('../config/xgboost_params.json')\n",
    "\n",
    "print(\"\\nModel Parameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model with Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = XGBRegressor(**params)\n",
    "\n",
    "# Train with evaluation sets\n",
    "print(\"Training XGBoost model with validation monitoring...\")\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PHASE 2: Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot learning curves\n",
    "results = model.evals_result()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(results['validation_0']['rmse'], label='Train RMSE', linewidth=2)\n",
    "plt.plot(results['validation_1']['rmse'], label='Validation RMSE', linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('XGBoost Learning Curve - Overfitting Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/learning_curve_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Learning curve saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on all sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated for train, validation, and test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics (with PHASE 2: Adjusted R¬≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<15} {'Train':<12} {'Validation':<12} {'Test':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'RMSE':<15} {train_rmse:<12.4f} {val_rmse:<12.4f} {test_rmse:<12.4f}\")\n",
    "print(f\"{'MAE':<15} {train_mae:<12.4f} {val_mae:<12.4f} {test_mae:<12.4f}\")\n",
    "print(f\"{'R¬≤':<15} {train_r2:<12.4f} {val_r2:<12.4f} {test_r2:<12.4f}\")\n",
    "\n",
    "# PHASE 2: Adjusted R¬≤\n",
    "test_adj_r2 = adjusted_r2(test_r2, len(y_test), X_test.shape[1])\n",
    "val_adj_r2 = adjusted_r2(val_r2, len(y_val), X_val.shape[1])\n",
    "\n",
    "print(\"\\nüìä PHASE 2: Adjusted R¬≤ (accounts for # of features):\")\n",
    "print(f\"  Test:       {test_adj_r2:.4f} (vs R¬≤: {test_r2:.4f})\")\n",
    "print(f\"  Validation: {val_adj_r2:.4f} (vs R¬≤: {val_r2:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization: Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Train\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.3, s=10)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Popularity')\n",
    "axes[0].set_ylabel('Predicted Popularity')\n",
    "axes[0].set_title(f'Training Set\\nR¬≤ = {train_r2:.4f}, RMSE = {train_rmse:.2f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.3, s=10, color='orange')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Popularity')\n",
    "axes[1].set_ylabel('Predicted Popularity')\n",
    "axes[1].set_title(f'Validation Set\\nR¬≤ = {val_r2:.4f}, RMSE = {val_rmse:.2f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.3, s=10, color='green')\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Popularity')\n",
    "axes[2].set_ylabel('Predicted Popularity')\n",
    "axes[2].set_title(f'Test Set\\nR¬≤ = {test_r2:.4f}, RMSE = {test_rmse:.2f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/all_sets_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. PHASE 2: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap with target\n",
    "analysis_df = X_train.copy()\n",
    "analysis_df['popularity'] = y_train.values\n",
    "\n",
    "# Calculate correlations with target\n",
    "correlations = analysis_df.corr()['popularity'].drop('popularity').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 Features Correlated with Popularity:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "# Plot top 20 correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_corr = correlations.abs().sort_values(ascending=False).head(20)\n",
    "colors = ['green' if correlations[f] > 0 else 'red' for f in top_corr.index]\n",
    "plt.barh(range(len(top_corr)), [correlations[f] for f in top_corr.index], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(top_corr)), top_corr.index)\n",
    "plt.xlabel('Correlation with Popularity')\n",
    "plt.title('Top 20 Feature Correlations with Popularity', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PHASE 2: Residual Analysis with QQ Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Residual scatter\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.3, s=10)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted Popularity')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual histogram\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ Plot (PHASE 2)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('QQ Plot (Normality Check)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/residual_analysis_complete.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.4f}\")\n",
    "print(f\"  Std:  {residuals.std():.4f}\")\n",
    "print(f\"  Min:  {residuals.min():.4f}\")\n",
    "print(f\"  Max:  {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Standard Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (Standard XGBoost):\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/standard_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PHASE 1 CRITICAL: SHAP Values for Interpretability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides:\n",
    "- Global feature importance (more reliable than tree-based importance)\n",
    "- Feature impact direction (positive/negative)\n",
    "- Individual prediction explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Computing SHAP values (this may take a few minutes)...\")\n",
    "\n",
    "# Use a sample if test set is very large\n",
    "X_test_shap = X_test\n",
    "if len(X_test) > 10000:\n",
    "    print(f\"  Sampling {10000} test samples for faster computation...\")\n",
    "    X_test_shap = X_test.sample(n=10000, random_state=42)\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test_shap)\n",
    "\n",
    "print(\"‚úÖ SHAP values computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 SHAP Summary Plot (Bar) - Global Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_shap, plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance (Global)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/shap_summary_bar_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 SHAP Beeswarm Plot - Feature Impact Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP beeswarm plot (shows feature impact direction)\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_shap, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Impact (Beeswarm)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/shap_beeswarm_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\\nüéØ How to read the beeswarm plot:\n",
    "- X-axis: SHAP value (impact on prediction)\n",
    "- Color: Feature value (red=high, blue=low)\n",
    "- Position: Each dot is one sample\n",
    "- Example: If high values of a feature (red dots) appear on the right,\n",
    "  it means that feature increases popularity when high.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Compare SHAP vs Standard Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean absolute SHAP values\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_test_shap.columns,\n",
    "    'shap_importance': np.abs(shap_values.values).mean(axis=0)\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "# Merge with standard importance\n",
    "comparison = shap_importance.merge(\n",
    "    feature_importance[['feature', 'importance']],\n",
    "    on='feature'\n",
    ")\n",
    "\n",
    "print(\"\\nTop 15 Features - SHAP vs Standard Importance:\")\n",
    "print(comparison.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "top_15_features = shap_importance.head(15)['feature'].values\n",
    "comparison_top = comparison[comparison['feature'].isin(top_15_features)].copy()\n",
    "\n",
    "# Normalize for comparison\n",
    "comparison_top['shap_norm'] = comparison_top['shap_importance'] / comparison_top['shap_importance'].max()\n",
    "comparison_top['xgb_norm'] = comparison_top['importance'] / comparison_top['importance'].max()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "x = np.arange(len(comparison_top))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, comparison_top['shap_norm'], width, label='SHAP', color='steelblue')\n",
    "ax.barh(x + width/2, comparison_top['xgb_norm'], width, label='XGBoost', color='orange')\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(comparison_top['feature'])\n",
    "ax.set_xlabel('Normalized Importance')\n",
    "ax.set_title('Feature Importance: SHAP vs XGBoost (Top 15)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/plots/shap_vs_standard_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 Example: Explain Individual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random sample to explain\n",
    "sample_idx = 42  # You can change this\n",
    "\n",
    "print(f\"\\nüìä Explaining prediction for sample {sample_idx}:\")\n",
    "print(f\"  Actual popularity: {y_test.iloc[sample_idx]:.2f}\")\n",
    "print(f\"  Predicted popularity: {y_test_pred[sample_idx]:.2f}\")\n",
    "print(f\"  Error: {abs(y_test.iloc[sample_idx] - y_test_pred[sample_idx]):.2f}\")\n",
    "\n",
    "# Waterfall plot for individual prediction\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.plots.waterfall(shap_values[sample_idx], show=False, max_display=15)\n",
    "plt.title(f'SHAP Explanation for Sample {sample_idx}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../outputs/plots/shap_waterfall_sample_{sample_idx}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\\nüéØ How to read the waterfall plot:\n",
    "- Base value: Average model prediction\n",
    "- Red bars: Features pushing prediction higher\n",
    "- Blue bars: Features pushing prediction lower\n",
    "- Final value (f(x)): Actual prediction for this sample\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. PHASE 1: Save Model with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Collect metrics\n",
    "metrics = {\n",
    "    'train_rmse': float(train_rmse),\n",
    "    'val_rmse': float(val_rmse),\n",
    "    'test_rmse': float(test_rmse),\n",
    "    'train_mae': float(train_mae),\n",
    "    'val_mae': float(val_mae),\n",
    "    'test_mae': float(test_mae),\n",
    "    'train_r2': float(train_r2),\n",
    "    'val_r2': float(val_r2),\n",
    "    'test_r2': float(test_r2),\n",
    "    'test_adjusted_r2': float(test_adj_r2),\n",
    "    'val_adjusted_r2': float(val_adj_r2)\n",
    "}\n",
    "\n",
    "# Create comprehensive metadata\n",
    "metadata = create_model_metadata(\n",
    "    model_params=params,\n",
    "    metrics=metrics,\n",
    "    feature_names=list(X_train.columns),\n",
    "    train_size=X_train.shape,\n",
    "    test_size=X_test.shape\n",
    ")\n",
    "\n",
    "# Add notebook-specific info\n",
    "metadata['source'] = 'notebook/04_Improved_ML_Pipeline.ipynb'\n",
    "metadata['improvements_implemented'] = {\n",
    "    'phase_1': ['data_validation', 'separate_validation_set', 'shap_values', 'git_tracking', 'environment_metadata'],\n",
    "    'phase_2': ['json_config', 'learning_curves', 'adjusted_r2', 'correlation_heatmap', 'qq_plots']\n",
    "}\n",
    "\n",
    "# Save model and metadata\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f'../outputs/models/improved_xgb_model_{timestamp}.joblib'\n",
    "metadata_path = f'../outputs/metadata/improved_xgb_metadata_{timestamp}.json'\n",
    "\n",
    "save_model_with_metadata(model, metadata, model_path, metadata_path)\n",
    "\n",
    "print(\"\\n‚úÖ Model and metadata saved with full reproducibility tracking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVED ML PIPELINE - EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Final Model Performance:\")\n",
    "print(f\"  Test R¬≤:          {test_r2:.4f}\")\n",
    "print(f\"  Test Adjusted R¬≤: {test_adj_r2:.4f}\")\n",
    "print(f\"  Test RMSE:        {test_rmse:.4f}\")\n",
    "print(f\"  Test MAE:         {test_mae:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 (Critical) Improvements Implemented:\")\n",
    "print(\"  ‚úì Data validation with feature consistency checks\")\n",
    "print(\"  ‚úì Separate validation set (no data leakage)\")\n",
    "print(\"  ‚úì SHAP values for model interpretability\")\n",
    "print(\"  ‚úì Git commit hash tracking\")\n",
    "print(\"  ‚úì Environment metadata capture\")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 (Enhanced) Improvements Implemented:\")\n",
    "print(\"  ‚úì JSON configuration management\")\n",
    "print(\"  ‚úì Learning curves (overfitting detection)\")\n",
    "print(\"  ‚úì Adjusted R¬≤ metric\")\n",
    "print(\"  ‚úì Correlation heatmap\")\n",
    "print(\"  ‚úì QQ plots for residual normality\")\n",
    "\n",
    "print(\"\\nüéØ Top 5 Most Important Features (SHAP):\")\n",
    "for i, row in shap_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['feature']}: {row['shap_importance']:.6f}\")\n",
    "\n",
    "print(\"\\nüíæ Outputs Saved:\")\n",
    "print(f\"  ‚úì Model: {model_path}\")\n",
    "print(f\"  ‚úì Metadata: {metadata_path}\")\n",
    "print(f\"  ‚úì Visualizations: ../outputs/plots/ (10+ plots)\")\n",
    "\n",
    "if metadata.get('git_commit'):\n",
    "    print(f\"\\nüìå Git Commit: {metadata['git_commit'][:8]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ IMPROVED PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
